# -*- coding: utf-8 -*-
"""Metrics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17yPIw6wcLd-SRn3JyY9reEcP9C6YXzVe
"""

import os
import numpy as np
import pandas as pd
import json
import string
from utilis import Vocab, Utter, Sentence
import spacy
import en_core_web_sm
nlp = en_core_web_sm.load()
from spacy.tokens import Token
#import enchant

#enchant_eng = enchant.Dict("en_US")

operator_verbs = set(["hai","pe","pene", "peena","pi", "dena", "rehna", "rehta", "kardiya", "kiye", "puchna", "karein", "dekho", "karo", "jao", "kara", "kra", "aayenge", "liyo", "liya", "dekhne", "dekhna", "dekhte", "hogaye", "chahiye", "thi", "kiya","rahi", "raha", "karna", "kar", "liye", "diya", "karlunga", "hogaya","gya","gaya","kaunsi","chlo","chli","lagna","lagi", "ki", "karli", "kardi","tha","aayi","aaya","aya", "aati","ati","ayi"])

class CM_nlp():
    def __init__(self, native_lexicon_file, eng_lexicon_file, ner_set_file, data_file, bigrams_file):
    #def __init__(self, data_file):  
        super().__init__()
        self.file_path= data_file
        self.conversation= []
        self.conversation_ids= []
        self.wordbigrams=None
        
        with open(bigrams_file, encoding='utf-8') as f:
            self.wordbigrams = f.read().split('\n')
        

        for w in range(len(self.wordbigrams)):
            self.wordbigrams[w]=self.wordbigrams[w].split() 
                  
           
        ## Needs to be added
        """
        pre_dict= json.load(open(stray_file,'r'))
        self.stray_chars= []
        for x in pre_dict.keys():
            self.stray_chars += pre_dict[x]
        """  
        
        self.native_lexicon= []
        self.eng_lexicon= []
        self.ner_set=[]

        self.native_lexicon=self.read_tag_file(native_lexicon_file)
        self.eng_lexicon=self.read_tag_file(eng_lexicon_file)
        self.ner_set=self.read_tag_file(ner_set_file, ner_others=True)
       

        self.total_vocab= Vocab()
        self.eng_vocab= Vocab()
        self.native_vocab = Vocab()
        self.other_vocab= Vocab()
        self.ner_vocab= Vocab()
        self.cm_eng_vocab= Vocab()

        #list of list of utterances
        self.total_utter= Utter().from_ref_utter(self.conversation)
        self.unique_utter= Utter.from_ref_utter(self.conversation)
        self.repeated_utter= Utter.from_ref_utter(self.conversation)
        self.other_utter= Utter.from_ref_utter(self.conversation)
        self.cm_utter = Utter.from_ref_utter(self.conversation)
        self.native_utter= Utter.from_ref_utter(self.conversation)
        self.eng_utter= Utter.from_ref_utter(self.conversation)


        ##List of sentences
        self.cm_sents=Sentence()
        self.all_sents=Sentence()
        self.eng_matrix=Sentence.from_ref_sent(self.cm_sents.sents)
        self.hindi_matrix=Sentence.from_ref_sent(self.cm_sents.sents)
        

    def read_tag_file(self, filepath, ner_others=False):
        file = open(filepath).read().split('\n')
        content = []
        if ner_others==False:
          for i in file:
            try:
                content.append(i.split('\t'))
            except:
                continue

        elif ner_others==True:
           for i in file:
             try:
               content.append(i.split('\t')[1])
             except:
               continue          
        return content


    def create_cm_sents(self):
        """
        Creating a list of code-mixed sentences to create a list of hindi matrix and english matrix senteneces
        MUST BE CALLED BEFORE ANY METRICS ON MATRIX LEVEL
        """

        for i in range(len(self.cm_utter.index_ref_utter)):
            doc=nlp(self.cm_utter.ref_utter[self.cm_utter.index_ref_utter[i][0]][self.cm_utter.index_ref_utter[i][1]])
            for sent in doc.sents:
              print()
              print([self.is_code_choice(token,self.cm_utter.actual_ids[i]) for token in sent if token.text!=' '])
              print()
              code_choice_list=[self.is_code_choice(token,self.cm_utter.actual_ids[i]) for token in sent if token.text!=' ']
              if self.get_utterance_type(code_choice_list )=='code_mixed':
                self.cm_sents.add_sent(self.remove_name(sent.text), self.cm_utter.actual_ids[i])

        self.eng_matrix=Sentence.from_ref_sent(self.cm_sents.sents)
        self.hindi_matrix=Sentence.from_ref_sent(self.cm_sents.sents)
        
        self.matrix_language_dist()      

    def native_x(self, i, actual_conv_id):
        _id = actual_conv_id
        words = nlp(i)
        tln = 0        
        for word in words:
            if self.is_code_choice(word, _id) == 'Hindi':
                tln += 1
        if tln > 0:
            return tln
        return self.N_x(i, actual_conv_id)

    def P_x(self, sent, actual_conv_id):
        _id = actual_conv_id
        words = nlp(sent)
        count = 0
        if len(words) == 1:
            return 0
        prev = ''
        for word in words:
            #word = word.text
            icc = self.is_code_choice(word, _id)
            if icc in ['English', 'Hindi']:
                if prev == '':
                    prev = icc
                elif prev != icc:
                    count += 1
                    prev = icc
        return count
    
    def delta_x(self, i, actual_conv_id, prev):
        if prev == '':
            prev = self.metric_lang_of_sents(i, actual_conv_id, ref_id_sent=-1, add=False)
        elif prev != self.metric_lang_of_sents(i, actual_conv_id, ref_id_sent=-1, add=False):
            return 1, prev
        return 0, prev
    
    def N_x(self, sent, actual_conv_id):
        _id = actual_conv_id
        words = nlp(sent)
        counter = 0
        for word in words:
            if self.is_code_choice(word, actual_conv_id) in ['English','Hindi']:
                counter += 1
        return counter

    def inner_func(self, i, actual_conv_id, prev):
        d_x, prev = self.delta_x(i, actual_conv_id, prev)
        N = self.N_x(i, actual_conv_id)
        if not N:
            return 0, prev
        eq = 1 + d_x - (self.native_x(i,actual_conv_id) + self.P_x(i, actual_conv_id))/N
        return eq, prev
    
    def max_tln(self, sent, actual_conv_id):
        _id =  actual_conv_id
        i = sent
        words = nlp(i)
        tln_hindi = 0
        tln_eng = 0
        for word in words:
            lang = self.is_code_choice(word, _id)
            if lang == 'Hindi':
                tln_hindi += 1
            if lang == 'English':
                tln_eng += 1
        return max(tln_eng, tln_hindi)

    def code_mixed_statistics(self):
        """
        Calculating CMI-Index (Cc and Cavg), and I-index)
        """
      
        for i in range(len(self.total_utter.index_ref_utter)):
              doc=nlp(self.total_utter.ref_utter[self.total_utter.index_ref_utter[i][0]][self.total_utter.index_ref_utter[i][1]])
              for sent in doc.sents:
              #print()
              #print([self.is_code_choice(token,self.cm_utter.actual_ids[i]) for token in sent if token.text!=' '])
              #print()
              #code_choice_list=[self.is_code_choice(token,self.cm_utter.actual_ids[i]) for token in sent if token.text!=' ']
              #if self.get_utterance_type(code_choice_list )=='code_mixed':
                  self.all_sents.add_sent(self.remove_name(sent.text), self.total_utter.actual_ids[i])

        ### 1. Cu(x) (Preliminary)
        Cu_list = []
        for i in range(self.all_sents.count()):
            sent=self.all_sents.sents[i][0]
            print("SENT: in cu_list")
            print(sent)
            if not sent:
                continue
            n_x = self.N_x(sent, self.all_sents.actual_ids[i])
            if n_x:
                result = ((n_x - self.max_tln(sent, self.all_sents.actual_ids[i]) + self.P_x(sent, self.all_sents.actual_ids[i]))/(2 * n_x)) * 100
            else:
                result = 0
            Cu_list.append(result)

        print("**** CU_LIST: ****")
        print(Cu_list)
        print()
        ##2. Cavg
        Cu_avg = sum(Cu_list)/len(Cu_list)
        print("**** CU_AVG: ****")
        print(Cu_avg)
        print()

        ##3. Cc
        sigma = 0
        prev = ''
        for i in range(self.all_sents.count()):
            ## utt=sent
            sent=self.all_sents.sents[i][0]
            if not sent:
                continue
            result = self.inner_func(sent, self.all_sents.actual_ids[i], prev)
            sigma += result[0]
            prev = result[1]
        sigma *= 0.5
        Cu_metric = (100/self.total_utter.count()) * (sigma + ((5/6)*(self.cm_utter.count())))
        print("**** CU_METRIC: ****")
        print(Cu_metric)
        print()

        ##4. I index
        sum_i_index = 0
        for i in range(len(self.conversation)):
            tmp_sum = 0
            len_conversation = 0
            for utter in self.conversation[i]:
                if utter:  
                    len_conversation += 1
                    tmp_sum += self.integration_index(utter, self.conversation_ids[i])
            avg_conversation = tmp_sum/len_conversation
            sum_i_index += avg_conversation
        avg_i_index = sum_i_index/len(self.conversation)
        print("**** AVG_I_INDEX: ****")
        print(avg_i_index)
        print()
        



    def integration_index(self, sent, actual_conv_id):
        _id = actual_conv_id
        i = sent
        words = nlp(i)
        count = 0
        if len(words) == 1:
            return 0
        prev = ''
        for word in words:
            #word = str(word)
            if self.is_code_choice(word, _id) in ['English', 'Hindi']:
                if prev == '':
                    prev = self.is_code_choice(word, _id)
                elif prev != self.is_code_choice(word, _id):
                    count += 1
                    prev = self.is_code_choice(word,_id)
        return count/(len(words) - 1)


    def insertions_and_alternations(self, eng_insertions_in_hindi=False, hin_insertions_in_english=False, alternations=False, insertions_distributions=False ):

        if eng_insertions_in_hindi==True:
           self.insertions(embedding_lang='English')
        if hin_insertions_in_english==True:
           self.insertions(embedding_lang='Hindi')
        if alternations==True:
           self.alternations(embedding_code='English')
           self.alternations(embedding_code='Hindi')
        if insertions_distributions==True:
           self.insertions_distributions(embedding_code="English")
           self.insertions_distributions(embedding_code="Hindi")
    

    def insertions_distributions(self, embedding_code="English"):
        """
        Classifying as single word insertion vs multi word insertions
        """

        single_word_insertions = set()
        multi_word_insertions = set()
    
        if embedding_code=='English':
            print("************ embedding code: ENGLISH:*********")
            for i in range(self.hindi_matrix.count()):
                sent_text=self.hindi_matrix.ref_sents[self.hindi_matrix.index_ref_sents[i][0]][self.hindi_matrix.index_ref_sents[i][1]]
                sent = nlp(sent_text)
                count,multi_ins = 0,False
                #print(utterance)
                for token in sent:
                    if self.is_code_choice(token, self.hindi_matrix.actual_ids[i]) == embedding_code:count+=1
                    else:count = 0
                    if count == 2:
                        multi_ins = True
                        #print('Multi word insertion')
                        multi_word_insertions.add(sent_text)
                        break
        
                if not multi_ins:
                    #print("Single Word Insertion")
                    single_word_insertions.add(sent_text)
                #print('------------')
            print(single_word_insertions)
            print("*******Multi word insertions: ********")
            print(multi_word_insertions)

        if embedding_code=='Hindi':
            print("************ embedding code: HINDI :*********")
            for i in range(self.eng_matrix.count()):
                sent_text=self.eng_matrix.ref_sents[self.eng_matrix.index_ref_sents[i][0]][self.eng_matrix.index_ref_sents[i][1]]
                sent = nlp(sent_text)
                count,multi_ins = 0,False
                #print(utterance)
                for token in sent:
                    if self.is_code_choice(token, self.eng_matrix.actual_ids[i]) == embedding_code:count+=1
                    else:count = 0
                    if count == 2:
                        multi_ins = True
                        #print('Multi word insertion')
                        multi_word_insertions.add(sent_text)
                        break
        
                if not multi_ins:
                    #print("Single Word Insertion")
                    single_word_insertions.add(sent_text)
                #print('------------')
            print(single_word_insertions)
            print("*******Multi word insertions: ********")
            print(multi_word_insertions)



    def alternations(self, embedding_code='English'):
        alternations = set()

        if embedding_code=='English':
           for i in range(self.hindi_matrix.count()):
               sent_text=self.hindi_matrix.ref_sents[self.hindi_matrix.index_ref_sents[i][0]][self.hindi_matrix.index_ref_sents[i][1]]
               sent = nlp(sent_text)
               count = 0
               for token in sent:
                   if self.is_code_choice(token, self.hindi_matrix.actual_ids[i]) == embedding_code:count+=1
                   else:count = 0 #Reset count
                   if count>2:
                      alternations.add(sent_text)
                      break
        
           print("********* alternations: ******") 
           return alternations 
            


        if embedding_code=='Hindi':
           for i in range(self.eng_matrix.count()):
               sent_text=self.eng_matrix.ref_sents[self.eng_matrix.index_ref_sents[i][0]][self.eng_matrix.index_ref_sents[i][1]]
               sent = nlp(sent_text)
               count = 0
               for token in sent:
                   if self.is_code_choice(token, self.eng_matrix.actual_ids[i]) == embedding_code:count+=1
                   else:count = 0 #Reset count
                   if count>2:
                      alternations.add(sent_text)
                      break

           print("********* alternations: ******") 
           print(alternations) 
           return alternations   
         


    def insertions(self,embedding_lang='English'):
        insertions = set()

        if embedding_lang=='English':
            for i in range(self.hindi_matrix.count()):
                alternation = False
            
                sent_text=self.hindi_matrix.ref_sents[self.hindi_matrix.index_ref_sents[i][0]][self.hindi_matrix.index_ref_sents[i][1]]
                sent = nlp(sent_text)
                count_insertion = 0
                for token in sent:
                #print(token.text + " -> " + token._.is_code_choice)
                    if self.is_code_choice(token, self.hindi_matrix.actual_ids[i]) == embedding_lang:count_insertion+=1
                    else:count_insertion = 0 #Reset count
                    if count_insertion>2:
                        alternation = True
                        break
                if not alternation:
                    insertions.add(sent_text)
            
            print(f"*************INSERTIONS: {len(insertions)}************")
            print(insertions)
            return insertions
        
        if embedding_lang=='Hindi':
            for i in range(self.eng_matrix.count()):
                alternation = False
            
                sent_text=self.eng_matrix.ref_sents[self.eng_matrix.index_ref_sents[i][0]][self.eng_matrix.index_ref_sents[i][1]]
                sent = nlp(sent_text)
                count_insertion = 0
                for token in sent:
                #print(token.text + " -> " + token._.is_code_choice)
                    if self.is_code_choice(token, self.hindi_matrix.actual_ids[i]) == embedding_lang:count_insertion+=1
                    else:count_insertion = 0 #Reset count
                    if count_insertion>2:
                        alternation = True
                        break
                if not alternation:
                    insertions.add(sent_text)
            
            print("*************INSERTIONS: ************")
            print(insertions)
            return insertions
          



    def embedding_into_matrix_distribution(self, embedding='Hindi', matrix='English',  save_words=True, visualization=False):
        """

        """
        if embedding=='Hindi' and matrix=='English':
           
            hindi_embedding_in_english_matrix_count={}
            hindi_embedding_in_english_matrix_sentences={}
        
        
            for i in range(self.eng_matrix.count()):
                hindi_embedding_count = 0
                sent = nlp(self.eng_matrix.ref_sents[self.eng_matrix.index_ref_sents[i][0]][self.eng_matrix.index_ref_sents[i][1]]) 
                sent_text=self.eng_matrix.ref_sents[self.eng_matrix.index_ref_sents[i][0]][self.eng_matrix.index_ref_sents[i][1]] 
                for word in sent: 
                    if self.is_code_choice(word, self.eng_matrix.actual_ids[i]) == "Hindi":
                        hindi_embedding_count += 1
                if hindi_embedding_count in hindi_embedding_in_english_matrix_count:
                    hindi_embedding_in_english_matrix_count[hindi_embedding_count]+=1
                else: 
                    hindi_embedding_in_english_matrix_count[hindi_embedding_count]=1

                if save_words==True:
                  if hindi_embedding_count in hindi_embedding_in_english_matrix_sentences:
                    hindi_embedding_in_english_matrix_sentences[hindi_embedding_count].append(sent_text)
                  else: 
                    hindi_embedding_in_english_matrix_sentences[hindi_embedding_count]=[sent_text]

                
          
            print("***** EMBEDDING DISTRIBUTION: ")
            print(hindi_embedding_in_english_matrix_count.items())
                ##hindi_embedding_in_english_matrix_count.append(hindi_embedding_count)

            print("***** EMBEDDING DISTRIBUTION WITH SENTENCES: ")
            print( hindi_embedding_in_english_matrix_sentences.items())
                ##hindi_embedding_in_english_matrix_count.append(hindi_embedding_count)


        if embedding=='English' and matrix=='Hindi':
           
            eng_embedding_in_hindi_matrix_count={}
            eng_embedding_in_hindi_matrix_sentences={}
        
        
            for i in range(self.hindi_matrix.count()):
                eng_embedding_count = 0
                sent = nlp(self.hindi_matrix.ref_sents[self.hindi_matrix.index_ref_sents[i][0]][self.hindi_matrix.index_ref_sents[i][1]]) 
                sent_text=self.hindi_matrix.ref_sents[self.hindi_matrix.index_ref_sents[i][0]][self.hindi_matrix.index_ref_sents[i][1]] 
                for word in sent: 
                    if self.is_code_choice(word, self.hindi_matrix.actual_ids[i]) == "English":
                        eng_embedding_count += 1
                if eng_embedding_count in eng_embedding_in_hindi_matrix_count:
                    eng_embedding_in_hindi_matrix_count[eng_embedding_count]+=1
                else: 
                    eng_embedding_in_hindi_matrix_count[eng_embedding_count]=1

                if save_words==True:
                  if eng_embedding_count in eng_embedding_in_hindi_matrix_sentences:
                    eng_embedding_in_hindi_matrix_sentences[eng_embedding_count].append(sent_text)
                  else: 
                    eng_embedding_in_hindi_matrix_sentences[eng_embedding_count]=[sent_text]

                
          
            print("***** EMBEDDING DISTRIBUTION: ")
            print(eng_embedding_in_hindi_matrix_count.items())
                ##hindi_embedding_in_english_matrix_count.append(hindi_embedding_count)

            print("***** EMBEDDING DISTRIBUTION WITH SENTENCES: ")
            print(eng_embedding_in_hindi_matrix_sentences.items())
                ##hindi_embedding_in_english_matrix_count.append(hindi_embedding_count)
                
            

    def k_nonnative_words_in_cm_sents(self, k=6, save_words=False, visualization=False):
        """
        k english words: in all codemixed sentences(including english and hindi matrix sentences)
        """
        k_non_native_dict = {}
        for i in range(k+1):
          k_non_native_dict[i]=0

        k_non_native_ls_utterances = {}
        for i in range(k+1):
          k_non_native_ls_utterances[i]=[]


      
        for i in range(self.cm_sents.count()):
          eng_word_count=0
          for word in nlp(self.cm_sents.sents[i][0]): 
            if (self.is_code_choice(word, self.cm_sents.actual_ids[i]) == 'English'):
                    eng_word_count+=1 
          if eng_word_count >= 1 and eng_word_count <=k:
              k_non_native_dict[eng_word_count] += 1
          if save_words==True:
              k_non_native_ls_utterances[eng_word_count].append(self.cm_sents.sents[i])
      

        print('**** K non-native(english) words distribution in the code-mixed corpora:')
        for i in range(len(k_non_native_dict.items())):
            print( list(k_non_native_ls_dict.items())[i])

        if visualization==True:
        
          import matplotlib.pyplot as plt
          fig = plt.subplots(figsize=(7,4))
          plt.bar(k_non_native_dict.keys(), k_non_native_dict.values(), 0.8, color="#000000")
          plt.ylabel("Number of utterances")
          plt.xlabel("K value")
          plt.title(f"Plotting {k} non native words in utterances in the corpus")

    

    def matrix_language_dist(self):
        
        for i in range(len(self.cm_sents.sents)):
          self.metric_lang_of_sents(self.cm_sents.sents[i][0], self.cm_sents.actual_ids[i], i)

    def metric_lang_of_sents(self, sent, actual_conv_id, ref_id_sent, add=True):
        
        operator_flag = 0
        bigram_flag = 0
        lang_code = []
        
        doc=nlp(sent)
        sentence_tokens=[ token for token in doc if token.text!=' ']
        for i in range(len(sentence_tokens)): 
            
            lang_code.append(self.is_code_choice(sentence_tokens[i], actual_conv_id))

            #Checking if word present in Hindi operator verbs
            if str(sentence_tokens[i].text) in operator_verbs and operator_flag!=1:
                operator_flag = 1

            #Checking if word satisfies bigram condition
            if i+1 < len(sentence_tokens) and bigram_flag!=1:
                #if str(utterance[i]) in wordbigrams_first:
                for w in range(len(self.wordbigrams)-1):
                  if self.wordbigrams[w][0]==sentence_tokens[i].text and self.wordbigrams[w][1]==sentence_tokens[i+1].text:
                    bigram_file=1
                    break

          

        #utterance = str(utterance)
        #Hindi Majority
        #length_sent=len[]
        
        if lang_code.count('Hindi') > len(lang_code)/2:
                if add==True:
                    self.hindi_matrix.add_ref_index((ref_id_sent, 0, len(sentence_tokens)), actual_conv_id)
                else:
                  return 'Hindi'
               
            
        #English Majority
        elif lang_code.count('English') > len(lang_code)/2:
             if operator_flag == 1:
                 if add==True:
                    self.hindi_matrix.add_ref_index((ref_id_sent, 0, len(sentence_tokens)), actual_conv_id)
                 else:
                    return 'Hindi'
             elif bigram_flag == 1:
                 if add==True:
                    self.hindi_matrix.add_ref_index((ref_id_sent, 0, len(sentence_tokens)), actual_conv_id)
                 else:
                    return 'Hindi'
             else:
                 if add==True:
                    self.eng_matrix.add_ref_index((ref_id_sent, 0, len(sentence_tokens)), actual_conv_id)
                 else:
                    return 'English'

        else:
            if operator_flag == 1:
                if add==True:
                    self.hindi_matrix.add_ref_index((ref_id_sent, 0, len(sentence_tokens)), actual_conv_id)
                else:
                    return 'Hindi'
            elif bigram_flag == 1:
                if add==True:
                    self.hindi_matrix.add_ref_index((ref_id_sent, 0, len(sentence_tokens)), actual_conv_id)
                else:   
                    return 'Hindi'
            else:
                if add==True:
                    self.eng_matrix.add_ref_index((ref_id_sent, 0, len(sentence_tokens)), actual_conv_id)
                else:    
                    return 'English'
    
    def is_code_choice(self, token, actual_conv_id):
        """
        This function checks each word type into 5 classes
        stary_chars - encoding != utf8
        self.eng_lexion and self.native_lexion -- lexicon words with frequencies
        
        """
        
        obj = str.maketrans('', '',string.punctuation)
        cleaned_text = str(token.text).translate(obj)

        ##Needs to be added
        #if token.text in emojis_set:
            #return 'Univ'
        #print(f'Cleaned_text: {cleaned_text}')

        if cleaned_text in self.ner_set:
            return 'NE'      
        for i in self.native_lexicon:
            if str(i[0])[:-2]==str(actual_conv_id) and i[1]==token.text:
              return 'Hindi'
        for i in self.eng_lexicon:
            if str(i[0])[:-2]==str(actual_conv_id) and i[1]==token.text:
              return 'English'

        ## Needs to be added
        #if token.text.lower() in stray_chars or token.is_punct:
           # return 'Univ'
        return 'Other'



    def remove_name(self, sentence):
        """
        this function removes the name of the speaker 
        """
        # sentence = sentence.text.strip()
        try:
            index = sentence.index(':')
        except ValueError:
            return sentence
        return sentence[ index + 1:]

    def compute_vocabs(self,doc, actual_conv_id, lang):
        """[summary]
        Args:
            doc (nlp(sent)): spacy processed sent
        """
        for word in doc:
            code_choice = self.is_code_choice(word, actual_conv_id)
            self.total_vocab.add_word(word.text)
            if code_choice =='English':
                if lang=='English':
                  self.eng_vocab.add_word(word.text)
                elif lang=='code_mixed':  
                  self.cm_eng_vocab.add_word(word.text)
            elif code_choice =='Hindi':
                self.native_vocab.add_word(word.text)
            elif code_choice == 'Other':
                self.other_vocab.add_word(word.text)
            elif code_choice == 'NE':
                self.ner_vocab.add_word(word.text)

    def get_utterance_type(self, code_choice_list):
        """
        This func filters punct, emojis etc.
        """
        print(f'code_choice_list: {code_choice_list}')
        filtered_list = []### example : "thank you ji" --- [english, english, hindi]
        for code in code_choice_list:
            if code in ['English', 'Hindi']:
                filtered_list.append(code)
        if filtered_list:
            utt_type = set(filtered_list) ### set will remove dupliactes  so, " english, hindi"
            if len(utt_type) == 2: ### so length will be 2
                return 'code_mixed'### hence its code-mixed
            return tuple(utt_type)[0]### using[0] returns string and not the set - for example : "thank you"---"english[0],english[1]"
        return None

    def get_utter_and_vocab(self,conv, actual_conv_id):
        """
        First function to be called:
        """

        dialogues= conv.split("\n")
        ## Writing the conversation into the list first
        self.conversation.append(dialogues)
        self.conversation_ids.append(actual_conv_id)

        assert len(self.conversation) == len(self.conversation_ids)

        curr_uttr= []
        for utter in dialogues:
            utter=self.remove_name(utter)
            doc= nlp(utter) ##Utterance
            
            curr_uttr.append(utter)

            code_choice_list= [self.is_code_choice(token,actual_conv_id) for token in doc if token.text!=' ']
            lang = self.get_utterance_type(code_choice_list)
            self.compute_vocabs(doc, actual_conv_id, lang )


            self.total_utter.add_ref_index((len(self.conversation)-1,len(curr_uttr)-1, len(code_choice_list)), actual_conv_id)

            if not lang:
              self.other_utter.add_ref_index((len(self.conversation)-1,len(curr_uttr)-1, len(code_choice_list)), actual_conv_id)
            elif lang == 'English':
                    # all english
              self.eng_utter.add_ref_index((len(self.conversation)-1,len(curr_uttr)-1,  len(code_choice_list)),  actual_conv_id )
            elif lang == 'Hindi':
                    # all hindi
              self.native_utter.add_ref_index((len(self.conversation)-1,len(curr_uttr)-1, len(code_choice_list)), actual_conv_id )
            elif lang == 'code_mixed':
                    # code_mixed
              self.cm_utter.add_ref_index((len(self.conversation)-1,len(curr_uttr)-1, len(code_choice_list)), actual_conv_id )
            else:
              self.other_utter.add_ref_index((len(self.conversation)-1,len(curr_uttr)-1, len(code_choice_list)), actual_conv_id )

            
     

    def basic_dataset_stats(self):
        """
        Calculating the following basic stats for the corpus:
        1. Average length of utterances in the corpus
        2. Average number of code-mixed utterances per conversation/dialog
        3. Percentage of code-mixed utterances in the corpus
        4. Percentage of hindi utterances in the corpus
        5. Percentage of english utterances in the corpus
        """
          
        sum_lengths = sum([i[2] for i in self.total_utterances.index_ref_utter])
        avg_len_utter = sum_lengths/self.total_utterances.count()
        print(f'********* Average length of utterances in the corpus: {avg_len_utter}')

        avg_cm_utter_per_conversation= self.cm_utter.count()/len(self.conversation)
        print(f'********* Average number of code-mixed utterances per converstaion: {avg_cm_utter_per_conversation}')

        percentage_cm_utter = self.cm_utter.count()/self.total_utter.count()
        print(f'********* Percentage of code-mixed utterances in the corpus: {percentage_cm_utter}')

        percentage_native_utter = self.native_utter.count()/self.total_utter.count()
        print(f'********* Percentage of code-mixed utterances in the corpus: {percentage_native_utter}')

        percentage_eng_utter = self.eng_utter.count()/self.total_utter.count()
        print(f'********* Percentage of code-mixed utterances in the corpus: {percentage_eng_utter}')



if __name__ == "__main__":
  codemixed=CM_nlp('../hindi_tags.txt',
                 '../english_tags.txt',
                 '../ne_tags.txt',
                 '../conversations_train_for_the_library.json',
                 '../hi-Latn-wordbigrams.txt')
  
  ## Needed to be implemented first (self.get_utter_and_vocab)
  with open('../conversations_train_for_the_library.json', 'r') as fp_json:
      data=json.load(fp_json)
      for i in data:
          codemixed.get_utter_and_vocab(i['conv'], i['id'])
  
  ##Needed for all the matrix level metrics
  codemixed.create_cm_sents()